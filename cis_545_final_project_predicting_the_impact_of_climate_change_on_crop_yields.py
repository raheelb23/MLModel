# -*- coding: utf-8 -*-
"""CIS 545 Final Project - Predicting the Impact of Climate Change on Crop Yields.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l86tskRhfI-bgji_mlcHhmfsNgV2EH_X

# CIS 545 Group Project

Group members: Muizz Mullani (ID #12657249
), Raheel Bhimani (ID #83805554), Jesús Peña (ID #85378431)

---

# Section 1: Installing and Importing Libraries
"""

from google.colab import drive

!pip3 install boto3
!pip install pandasql

import numpy as np 
import json
import matplotlib
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import cm
from datetime import datetime
import glob
import seaborn as sns
import re
import os
import pandasql as ps #SQL on Pandas Dataframe
from pandasql import sqldf
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

!apt install libkrb5-dev
!wget https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz
!tar xf spark-2.4.5-bin-hadoop2.7.tgz
!pip install findspark
!pip install sparkmagic
!pip install pyspark
!pip install pyspark --user
!pip install seaborn --user
!pip install plotly --user
!pip install imageio --user
!pip install folium --user

!apt update
!apt install gcc python-dev libkrb5-dev

from pyspark.sql import SparkSession
from pyspark.sql.types import *
import pyspark.sql.functions as F

import os

spark = SparkSession.builder.appName('mcit545-final_project').getOrCreate()

"""# Section 2: Importing Raw Data Files and Processing

Sources: 

*   https://ourworldindata.org/crop-yields#licence
*   https://www.kaggle.com/noaa/noaa-global-surface-summary-of-the-day
*   https://ourworldindata.org/co2-emissions

---





"""

from pyspark import SparkFiles

url_country_codes = 'https://raw.githubusercontent.com/jesuspena91/mcit545_final_project/main/country_codes.csv'
url_crop = 'https://raw.githubusercontent.com/jesuspena91/mcit545_final_project/main/crop_output_data.csv'
url_co2 = 'https://raw.githubusercontent.com/jesuspena91/mcit545_final_project/main/annual-co2-emissions-per-country.csv'

country_codes_df = pd.read_csv(url_country_codes).dropna()
raw_crops_df = pd.read_csv(url_crop)
raw_co2_emissions_df = pd.read_csv(url_co2)

raw_crops_df

raw_co2_emissions_df

# Mounting drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Getting all weather data from Google Drive folder

path = '/content/gdrive/MyDrive/weather_subset'
file_paths = os.listdir(path)
temp_list = []

for file in file_paths:
  temp_path = path + "/" + file
  temp_df = pd.read_csv(temp_path, index_col=None, header=0)
  temp_list.append(temp_df)

raw_weather_df = pd.concat(temp_list, axis=0, ignore_index=True)

# Selecting a subset of columns
# Reference: https://www.ncei.noaa.gov/data/global-summary-of-the-year/doc/GSOY_documentation.pdf
# https://www.ncdc.noaa.gov/cdo-web/datasets
#TAVG - annual temperature average
#PRCP - total annual precipitation 
raw_weather_all_features_df = raw_weather_df[["STATION", "DATE", "TAVG", "PRCP"]].dropna()
print(raw_weather_all_features_df)

# Formatting the weather data frame
from pandasql import sqldf

# Obtaining the country for each station
pysqldf = lambda q: sqldf(q, globals())

q = """WITH temp_data AS (
       SELECT SUBSTR(STATION, 1 , 2) AS code2, DATE AS Year, *
       FROM raw_weather_all_features_df 
       )
       SELECT cd.country AS country, *
       FROM temp_data td
       JOIN country_codes_df cd ON td.code2 = cd.code2

       ;"""

raw_weather_by_country_df = pysqldf(q)
raw_weather_by_country_df

# Visualizing the weather data for some countries

q = """WITH temp_data AS (
       SELECT country, Year, TAVG
       FROM raw_weather_by_country_df 
       WHERE country IN ('Mexico','Canada')
       )
       SELECT *
       FROM temp_data td

       ;"""

subset_raw_weather_by_country_df = pysqldf(q)

# The plot seems to make sense: colder weather and more observations registered in Canada than in Mexico
sns.lmplot('Year', 'TAVG', data=subset_raw_weather_by_country_df, height=4, aspect=1.5, fit_reg=False, hue="country")

# Calculating averages at country level

q = """SELECT country, code3,Year, AVG(TAVG) AS TAVG, AVG(PRCP) AS PRCP
       FROM raw_weather_by_country_df
       GROUP BY 1,2,3
       ;"""

weather_df = pysqldf(q)
print(weather_df)

# Validating if global warming is observed in the data. We see at least slight increases in most countries

q = """WITH temp_data AS (
       SELECT *
       FROM weather_df 
       WHERE Year > 1980
       )
       SELECT *
       FROM temp_data td

       ;"""

subset_weather_df = pysqldf(q)

fig = plt.figure(figsize=(20, 4))
plt.plot('Year', 'TAVG', data=subset_weather_df[subset_weather_df['country'] == 'Mexico'], color='green', label="Mexico")
plt.plot('Year', 'TAVG', data=subset_weather_df[subset_weather_df['country'] == 'Canada'], color='red', label="Canada")
plt.plot('Year', 'TAVG', data=subset_weather_df[subset_weather_df['country'] == 'France'], color='blue', label="France")
plt.plot('Year', 'TAVG', data=subset_weather_df[subset_weather_df['country'] == 'Switzerland'], color='purple', label="Switzerland")
plt.legend(loc='best')

plt.ylabel('Average Temperature (log scaled)', fontsize = 10)
plt.xlabel('Year', fontsize = 10)
plt.yscale('log')

# Selecting a subset of columns from crops and co2 data frames
crops_df = raw_crops_df[["Entity", "Year", "wheat_attainable", "wheat_yield_gap"]]
crops_df_2 = raw_crops_df[["Entity", "Year", "maize_attainable", "maize_yield_gap"]]
co2_emissions_df = raw_co2_emissions_df

# Joining co2 and crops data based on country and year

q = '''SELECT 
            CAST(crop.Entity AS VARCHAR) AS country,
            CAST(co2.Code AS VARCHAR) AS code3,
            CAST(crop.Year AS INT64) AS Year,
            CAST(crop.wheat_attainable AS DOUBLE) AS wheat_attainable,
            CAST(crop.wheat_yield_gap AS DOUBLE) AS wheat_yield_gap,
            CAST(IFNULL(CAST(crop.wheat_attainable AS DOUBLE),0) - IFNULL(CAST(crop.wheat_yield_gap AS DOUBLE),0) AS DOUBLE) AS wheat_output,
            CAST(co2.co2_emissions AS DOUBLE) AS co2_emissions
           
           FROM crops_df crop
           JOIN co2_emissions_df co2 ON crop.Entity = co2.Entity AND crop.Year = co2.Year'''

consolidated_crop_co2_df = pysqldf(q)
print(consolidated_crop_co2_df)

q2 = '''SELECT 
            CAST(crop.Entity AS VARCHAR) AS country,
            CAST(co2.Code AS VARCHAR) AS code3,
            CAST(crop.Year AS INT64) AS Year,
            CAST(crop.maize_attainable AS DOUBLE) AS maize_attainable,
            CAST(crop.maize_yield_gap AS DOUBLE) AS maize_yield_gap,
            CAST(IFNULL(CAST(crop.maize_attainable AS DOUBLE),0) - IFNULL(CAST(crop.maize_yield_gap AS DOUBLE),0) AS DOUBLE) AS maize_output,
            CAST(co2.co2_emissions AS DOUBLE) AS co2_emissions
           
           FROM crops_df_2 crop
           JOIN co2_emissions_df co2 ON crop.Entity = co2.Entity AND crop.Year = co2.Year'''

consolidated_crop_co2_df_2 = pysqldf(q2)
print(consolidated_crop_co2_df_2)

# Visualizing crop output data
# wheat

q = """WITH temp_data AS (
       SELECT *
       FROM consolidated_crop_co2_df 
       WHERE Year > 1980
       )
       SELECT *
       FROM temp_data td

       ;"""

subset_consolidated_df = pysqldf(q)

fig = plt.figure(figsize=(20, 4))
plt.plot('Year', 'wheat_output', data=subset_consolidated_df[subset_consolidated_df['country'] == 'Mexico'], color='green', label="Mexico")
plt.plot('Year', 'wheat_output', data=subset_consolidated_df[subset_consolidated_df['country'] == 'Canada'], color='red', label="Canada")
plt.plot('Year', 'wheat_output', data=subset_consolidated_df[subset_consolidated_df['country'] == 'France'], color='blue', label="France")
plt.plot('Year', 'wheat_output', data=subset_consolidated_df[subset_consolidated_df['country'] == 'Switzerland'], color='purple', label="Switzerland")
plt.legend(loc='best')

plt.ylabel('Wheat Output', fontsize = 10)
plt.xlabel('Year', fontsize = 10)

# Visualizing crop output data
# maize

q = """WITH temp_data AS (
       SELECT *
       FROM consolidated_crop_co2_df_2 
       WHERE Year > 1980
       )
       SELECT *
       FROM temp_data td

       ;"""

subset_consolidated_df = pysqldf(q)

fig = plt.figure(figsize=(20, 4))
plt.plot('Year', 'maize_output', data=subset_consolidated_df[subset_consolidated_df['country'] == 'Mexico'], color='green', label="Mexico")
plt.plot('Year', 'maize_output', data=subset_consolidated_df[subset_consolidated_df['country'] == 'Canada'], color='red', label="Canada")
plt.plot('Year', 'maize_output', data=subset_consolidated_df[subset_consolidated_df['country'] == 'France'], color='blue', label="France")
plt.plot('Year', 'maize_output', data=subset_consolidated_df[subset_consolidated_df['country'] == 'Switzerland'], color='purple', label="Switzerland")
plt.legend(loc='best')

plt.ylabel('Maize Output', fontsize = 10)
plt.xlabel('Year', fontsize = 10)

# Joining weather data into consolidated data frame

q = '''SELECT 
            c.country AS country,
            c.code3 AS code3,
            c.Year AS Year,
            wheat_attainable,
            wheat_yield_gap,
            wheat_output,
            co2_emissions,
            TAVG,
            PRCP

           FROM consolidated_crop_co2_df c
           JOIN weather_df w ON c.code3 = w.code3 AND c.Year = w.Year
           '''

consolidated_df = pysqldf(q)
consolidated_df

q2 = '''SELECT 
            c.country AS country,
            c.code3 AS code3,
            c.Year AS Year,
            maize_attainable,
            maize_yield_gap,
            maize_output,
            co2_emissions,
            TAVG,
            PRCP

           FROM consolidated_crop_co2_df_2 c
           JOIN weather_df w ON c.code3 = w.code3 AND c.Year = w.Year
           '''

consolidated_df_2 = pysqldf(q2)
consolidated_df_2

"""## Section 3: Modeling and Accuracy Analysis

**Part 1: Getting dataframe ready to be split into features and labels for maching learning**
"""

#Drop columns that are not needed
consolidated_lean_df = consolidated_df.drop(['code3','wheat_attainable','wheat_yield_gap'], axis = 1)
consolidated_lean_df

consolidated_lean_df_2 = consolidated_df_2.drop(['code3','maize_attainable','maize_yield_gap'], axis = 1)
consolidated_lean_df_2

#Create one hot vectors for categorical data
consolidated_lean_df.country = consolidated_lean_df.country.astype('category')
consolidated_lean_df.Year = consolidated_lean_df.country.astype('category')

consolidated_lean_df = pd.get_dummies(consolidated_lean_df, columns=['country', 'Year'])
consolidated_lean_df

consolidated_lean_df_2.country = consolidated_lean_df_2.country.astype('category')
consolidated_lean_df_2.Year = consolidated_lean_df_2.country.astype('category')

consolidated_lean_df_2 = pd.get_dummies(consolidated_lean_df_2, columns=['country', 'Year'])
consolidated_lean_df_2

#Create features and labels

features = consolidated_lean_df.drop(['wheat_output'], axis = 1)
label = consolidated_lean_df['wheat_output']

features_2 = consolidated_lean_df_2.drop(['maize_output'], axis = 1)
label_2 = consolidated_lean_df_2['maize_output']

"""**Part 2: Now, we are ready to complete pre-processing steps of standardizing the features and running PCA.**"""

#Standardize the features
features_std = StandardScaler().fit_transform(features)
features_std

features_std_2 = StandardScaler().fit_transform(features_2)
features_std_2

#Split into train and test datasets
#wheat
x = features_std
y = np.array(label)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20)

#maize
x_2 = features_std_2
y_2 = np.array(label_2)

x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x_2, y_2, test_size=0.20)

#Run PCA to reduce dimensionality
pca = PCA(n_components=len(features.columns))
pca.fit(x_train)

percentage_var_explained = pca.explained_variance_ratio_  
cum_var_explained=np.cumsum(percentage_var_explained)
  
plt.figure(1,figsize=(6,4))
plt.clf()  
plt.plot(cum_var_explained,linewidth=2)  
plt.axis('tight')  
plt.grid() 
plt.xlabel('n_components') 
plt.ylabel('Cumulative_Variance_explained')  
plt.title('Wheat PCA')
plt.show()


#We learn that about 7 components are enough for wheat

#Run PCA to reduce dimensionality
pca = PCA(n_components=len(features_2.columns))
pca.fit(x_train_2)

percentage_var_explained = pca.explained_variance_ratio_  
cum_var_explained=np.cumsum(percentage_var_explained)
  
plt.figure(1,figsize=(6,4))
plt.clf()  
plt.plot(cum_var_explained,linewidth=2)  
plt.axis('tight')  
plt.grid() 
plt.xlabel('n_components') 
plt.ylabel('Cumulative_Variance_explained')  
plt.title('Maize PCA')
plt.show()


#We learn that about 7 components are enough for maize

#Now apply what we learned from PCA to train and test sets 
#wheat
pca = PCA(n_components=7)
x_train = pca.fit_transform(x_train)
x_test = pca.transform(x_test)

#maize
pca = PCA(n_components=7)
x_train_2 = pca.fit_transform(x_train_2)
x_test_2 = pca.transform(x_test_2)

"""**Part 3: Building and evaluating linear regression and random forest models**"""

#Linear Regression
#wheat
model = LinearRegression().fit(x_train,y_train)
y_pred = model.predict(x_test)

mse_test = mean_squared_error(y_test,y_pred)
r2_test = r2_score(y_test,y_pred)

print("Mean Squared Error: %.2f" %mse_test)
print("Coefficient of Determination: %.2f" %r2_test)

#Tuning parameters using GridSearch and Random Forest as the estimator
#wheat
rfr = RandomForestRegressor()
params = {'max_depth': [25,40,52,65,75],'n_estimators': [10,30,50,100,150]}

search = GridSearchCV(estimator=rfr,param_grid = params, n_jobs = -1)
search.fit(x_train,y_train)
search.cv_results_
search.best_params_

#Running Random Forest Regression
#wheat
rfr = RandomForestRegressor(n_estimators=10,max_depth=25)
rfr.fit(x_train,y_train)
y_pred = rfr.predict(x_test)

mse_test = mean_squared_error(y_test,y_pred)
r2_test = r2_score(y_test,y_pred)

print("Mean Squared Error: %.2f" %mse_test)
print("Coefficient of Determination: %.2f" %r2_test)

#Linear Regression
#maize
model = LinearRegression().fit(x_train_2,y_train_2)
y_pred_2 = model.predict(x_test_2)

mse_test = mean_squared_error(y_test_2,y_pred_2)
r2_test = r2_score(y_test_2,y_pred_2)

print("Mean Squared Error: %.2f" %mse_test)
print("Coefficient of Determination: %.2f" %r2_test)

#Tuning parameters using GridSearch and Random Forest as the estimator
#maize
rfr = RandomForestRegressor()
params = {'max_depth': [25,40,52,65,75],'n_estimators': [10,30,50,100,150]}

search = GridSearchCV(estimator=rfr,param_grid = params, n_jobs = -1)
search.fit(x_train_2,y_train_2)
search.cv_results_
search.best_params_

#Running Random Forest Regression
#maize
rfr = RandomForestRegressor(n_estimators=100,max_depth=40)
rfr.fit(x_train_2,y_train_2)
y_pred_2 = rfr.predict(x_test_2)

mse_test = mean_squared_error(y_test_2,y_pred_2)
r2_test = r2_score(y_test_2,y_pred_2)

print("Mean Squared Error: %.2f" %mse_test)
print("Coefficient of Determination: %.2f" %r2_test)

"""In the last run before turning in the project, we can see by using random forest regression vs linear regression, 

*   For wheat: we can reduce the MSE from 1.32 to 0.50 and increasing the r2 score from 0.72 to 0.90
*   For maize: we can reduce the MSE from 2.75 to 1.23 and increasing the r2 score from 0.69 to 0.86

"""